{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA has an execution model unlike the traditional sequential model used for programming CPUs. In CUDA, the code you write will be executed by multiple threads at once (often hundreds or thousands). Your solution will be modeled by defining a thread **hierarchy of grid, blocks and threads**.\n",
    "- CPU: sequential model\n",
    "- CUDA: multiple threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# -------------------\n",
    "import numba\n",
    "import numba.cuda as cuda\n",
    "print(numba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kernel declaration\n",
    "**A kernel function is a GPU function that is meant to be called from CPU code** (*). It gives it two fundamental characteristics:\n",
    "- kernels cannot explicitly return a value\n",
    "- kernels explicitly declare their thread hierarchy when called\n",
    "\n",
    "(*) Note: newer CUDA devices support device-side kernel launching; this feature is called dynamic parallelism but Numba does not support it currently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def increment_by_one(an_array):\n",
    "    \"\"\"\n",
    "    Increment all array elements by one.\n",
    "    \"\"\"\n",
    "    # code elided here; read further for different implementations\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kernel invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kernel is typically launched in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_array = np.random.random(63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "threadsperblock = 32\n",
    "blockspergrid = (an_array.size + (threadsperblock - 1)) // threadsperblock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kernel[number of blocks per grid, number of threads per block]` is like a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.compiler.AutoJitCUDAKernel at 0x7f2026952a20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "increment_by_one[blockspergrid, threadsperblock]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**two steps**  \n",
    "- Instantiate the kernel proper, by specifying a number of blocks (or “blocks per grid”), and a number of threads per block.\n",
    "    - total threads launched = threadsperblock x blockspergrid\n",
    "- Running the kernel, by passing it the input array (and any separate output arrays if necessary). By default, **running a kernel is synchronous**: the function returns when the kernel has finished executing and the data is synchronized back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the block size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a two-level hierarchy when declaring the number of threads needed by a kernel\n",
    "\n",
    "- On the software side: **how many threads share a given area of shared memory**\n",
    "- On the hardware side: **the block size must be large enough for full occupation of execution units**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-dimensional blocks and grids\n",
    "To help deal with multi-dimensional arrays, CUDA allows you to specify multi-dimensional blocks and grids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Thread positioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running a kernel, the kernel function’s code is executed by every thread once. It therefore has to know which thread it is in, in order to know which array element(s) it is responsible for (complex algorithms may define more complex responsibilities, but the underlying principle is the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way is for the thread to determines its position in the grid and block and manually compute the corresponding array position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def increment_by_one(an_array):\n",
    "    \n",
    "    tx = cuda.threadIdx.x    # Thread id in a 1D block\n",
    "    ty = cuda.blockIdx.x     # Block id in a 1D grid\n",
    "    bw = cuda.blockDim.x     # Block width, i.e. number of threads per block\n",
    "    \n",
    "    pos = tx + ty * bw       # Compute flattened index inside the array\n",
    "    if pos < an_array.size:  # Check array boundaries\n",
    "        an_array[pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "an_array = np.arange(10)\n",
    "\n",
    "# ============================\n",
    "threadsperblock = 32\n",
    "blockspergrid = (an_array.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# ============================\n",
    "print(an_array)\n",
    "increment_by_one[blockspergrid, threadsperblock](an_array)\n",
    "print(an_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: From below code, the idea is like allocating 2 dimensional memory in C/C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def get_blockDim(an_array):\n",
    "    \n",
    "    tx = cuda.threadIdx.x    # Thread id in a 1D block\n",
    "    ty = cuda.blockIdx.x     # Block id in a 1D grid\n",
    "    bw = cuda.blockDim.x     # Block width, i.e. number of threads per block\n",
    "    \n",
    "    pos = tx + ty * bw       # Compute flattened index inside the array\n",
    "    if pos < an_array.size:  # Check array boundaries\n",
    "        an_array[pos] = bw\n",
    "        \n",
    "@cuda.jit\n",
    "def get_pos(an_array):\n",
    "    \n",
    "    tx = cuda.threadIdx.x    # Thread id in a 1D block\n",
    "    ty = cuda.blockIdx.x     # Block id in a 1D grid\n",
    "    bw = cuda.blockDim.x     # Block width, i.e. number of threads per block\n",
    "    \n",
    "    pos = tx + ty * bw       # Compute flattened index inside the array\n",
    "    if pos < an_array.size:  # Check array boundaries\n",
    "        an_array[pos] = pos\n",
    "        \n",
    "@cuda.jit\n",
    "def get_threadIdx(an_array):\n",
    "    \n",
    "    tx = cuda.threadIdx.x    # Thread id in a 1D block\n",
    "    ty = cuda.blockIdx.x     # Block id in a 1D grid\n",
    "    bw = cuda.blockDim.x     # Block width, i.e. number of threads per block\n",
    "    \n",
    "    pos = tx + ty * bw       # Compute flattened index inside the array\n",
    "    if pos < an_array.size:  # Check array boundaries\n",
    "        an_array[pos] = tx\n",
    "        \n",
    "@cuda.jit\n",
    "def get_blockIdx(an_array):\n",
    "    \n",
    "    tx = cuda.threadIdx.x    # Thread id in a 1D block\n",
    "    ty = cuda.blockIdx.x     # Block id in a 1D grid\n",
    "    bw = cuda.blockDim.x     # Block width, i.e. number of threads per block\n",
    "    \n",
    "    pos = tx + ty * bw       # Compute flattened index inside the array\n",
    "    if pos < an_array.size:  # Check array boundaries\n",
    "        an_array[pos] = ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Block Dimension =====\n",
      "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
      "\n",
      "===== position =====\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n",
      "\n",
      "===== Thread Indices =====\n",
      "[0. 1. 2. 3. 0. 1. 2. 3. 0. 1. 2. 3. 0.]\n",
      "\n",
      "===== Block Indices =====\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "an_array = np.zeros(13)\n",
    "\n",
    "# ============================\n",
    "threadsperblock = 4\n",
    "blockspergrid = (an_array.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# ============================\n",
    "\n",
    "print(\"\\n===== Block Dimension =====\")\n",
    "get_blockDim[blockspergrid, threadsperblock](an_array)\n",
    "print(an_array)\n",
    "\n",
    "print(\"\\n===== position =====\")\n",
    "get_pos[blockspergrid, threadsperblock](an_array)\n",
    "print(an_array)\n",
    "\n",
    "print(\"\\n===== Thread Indices =====\")\n",
    "get_threadIdx[blockspergrid, threadsperblock](an_array)\n",
    "print(an_array)\n",
    "\n",
    "print(\"\\n===== Block Indices =====\")\n",
    "get_blockIdx[blockspergrid, threadsperblock](an_array)\n",
    "print(an_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple algorithms will tend to always use thread indices in the same way as shown in the example above. Numba provides additional facilities to automate such calculations:\n",
    "- numba.cuda.grid(ndim)\n",
    "- numba.cuda.gridsize(ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite example for 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def increment_by_one(an_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < an_array.size:\n",
    "        an_array[pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "an_array = np.arange(10)\n",
    "\n",
    "# ============================\n",
    "threadsperblock = 32\n",
    "blockspergrid = (an_array.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# ============================\n",
    "print(an_array)\n",
    "increment_by_one[blockspergrid, threadsperblock](an_array)\n",
    "print(an_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same example for a 2D array and grid of threads would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def increment_a_2D_array(an_array):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x < an_array.shape[0] and y < an_array.shape[1]:\n",
    "        an_array[x, y] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]]\n"
     ]
    }
   ],
   "source": [
    "an_array = np.arange(20).reshape(2, -1)\n",
    "print(an_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]]\n",
      "\n",
      "[[ 2  3  4  5  6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17 18 19 20 21]]\n"
     ]
    }
   ],
   "source": [
    "# now we need to define threadsperblock and blockspergrid for two dimension\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = math.ceil(an_array.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(an_array.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "print(an_array)\n",
    "increment_a_2D_array[blockspergrid, threadsperblock](an_array)\n",
    "print()\n",
    "print(an_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
