{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Link: https://www.youtube.com/watch?v=06VErVj9MaQ&t=1108s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba derives from \"Numpy\" and \"Mamba\". Numba turns Python into a compiled language with a GPU target.\n",
    "\n",
    "[You cannot use the python list and dictionary](https://numba.pydata.org/numba-doc/dev/cuda/cudapysupported.html). If you write in that way, it might be slower. But you can use Numpy array.\n",
    "```\n",
    "The following Python constructs are not supported:\n",
    "- Exception handling\n",
    "- context management (the with statement)\n",
    "- Comprehensions (either list, dict, set or generator comprehensions)\n",
    "- Generator (any yield statments)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Opernsource BSD license\n",
    "- Basic CUDA GPU JIT compilation\n",
    "- OpenCL support coming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba 0.34.0\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "print(\"numba\", numba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CUDA GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A massively parallel processor (many cores)\n",
    "    - 100 threads, 1000 threads, and more\n",
    "- optimized for data throughput\n",
    "    - simple (shallow) cache hierarchy\n",
    "    - best with manual caching!\n",
    "    - Cache memory is called shared memory and it is addressable\n",
    "- CPU is latency optimized\n",
    "    - Deep cache hierarchy\n",
    "    - L1, L2 L3 cahces\n",
    "- GPU execution model is different\n",
    "- GPU forces you to think and program in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: b'GeForce GTX 1080 Ti'\n"
     ]
    }
   ],
   "source": [
    "my_gpu = numba.cuda.get_current_device()\n",
    "print(\"Running on GPU:\", my_gpu.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute capability:  6.1 (Numba requires >= 2.0)\n"
     ]
    }
   ],
   "source": [
    "cc = my_gpu.compute_capability\n",
    "print(\"Compute capability: \", \"%d.%d\" % cc, \"(Numba requires >= 2.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of streaming multiprocessor: 28\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of streaming multiprocessor:\", my_gpu.MULTIPROCESSOR_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level Array-Oriented Style\n",
    "- Use NumPy array as a unit of computation\n",
    "- Use NumPy universal function (ufunc) as an abstraction of computation of scheduling\n",
    "- ufuncs are elementwise functions\n",
    "- If you use NumPy, you are using ufuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ufunc 'sin'> is of type <class 'numpy.ufunc'>\n",
      "<ufunc 'add'> is of type <class 'numpy.ufunc'>\n"
     ]
    }
   ],
   "source": [
    "print(np.sin, \"is of type\", type(np.sin))\n",
    "print(np.add, \"is of type\", type(np.add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize\n",
    "- generate a ufunc from a python function\n",
    "- converts scalar function to elementwise array function\n",
    "- Numba provides CPU support\n",
    "-  <s>NumbaPro provides GPU support</s>\n",
    "    - https://docs.anaconda.com/numbapro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU version\n",
    "@numba.vectorize(['float32(float32, float32)', \n",
    "                  'float64(float64, float64)'], target = 'cpu')\n",
    "def cpu_sincos(x, y):\n",
    "    return math.sin(x) * math.cos(y)\n",
    "\n",
    "# CPU version (multicore)\n",
    "@numba.vectorize(['float32(float32, float32)', \n",
    "                  'float64(float64, float64)'], target = 'parallel')\n",
    "def cpu_sincos_mc(x, y):\n",
    "    return math.sin(x) * math.cos(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "- https://numba.pydata.org/numba-doc/latest/cuda/ufunc.html\n",
    "- https://numba.pydata.org/numba-doc/dev/user/vectorize.html\n",
    "\n",
    "```\n",
    "The vectorize() decorator supports multiple ufunc targets:\n",
    "Target      Description\n",
    "cpu         Single-threaded CPU\n",
    "parallel    Multi-core CPU\n",
    "cuda        CUDA GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU version\n",
    "@numba.vectorize(['float32(float32, float32)', \n",
    "                  'float64(float64, float64)'], target = 'cuda')\n",
    "def gpu_sincos(x, y):\n",
    "    return math.sin(x) * math.cos(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Types and signatures in Numba\n",
    "- https://numba.pydata.org/numba-doc/dev/reference/types.html\n",
    "```\n",
    "An example function signature would be the string \"f8(i4, i4)\" (or the equivalent \"float64(int32, int32)\") which specifies a function taking two 32-bit integers and returning a double-precision float.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test it out\n",
    "- 2 input arrays\n",
    "- 1 output array\n",
    "- 1 million doubles (8 MB) per array\n",
    "- Total 24 MB of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: [numpy.allclose](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.allclose.html)  \n",
    "Returns True if two arrays are element-wise equal within a tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU vectorize correct:  True\n",
      "CPU vectorize correct:  True\n",
      "GPU vectorize correct:  True\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "n = 1000000\n",
    "x = np.linspace(0, np.pi, n)\n",
    "y = np.linspace(0, np.pi, n)\n",
    "\n",
    "# check result\n",
    "np_ans = np.sin(x) * np.cos(y)\n",
    "np_cpu_ans = cpu_sincos(x, y)\n",
    "np_cpumc_ans = cpu_sincos_mc(x, y)\n",
    "np_gpu_ans = gpu_sincos(x, y)\n",
    "\n",
    "\n",
    "print(\"CPU vectorize correct: \", np.allclose(np_cpu_ans, np_ans))\n",
    "print(\"CPU vectorize correct: \", np.allclose(np_cpumc_ans, np_ans))\n",
    "print(\"GPU vectorize correct: \", np.allclose(np_gpu_ans, np_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: calculate the time of each method.\n",
    "**Results:** \n",
    "- CPU vectorize time is similar to pure Numpy time because sin() and cos() calls dominate the time\n",
    "- CPU vectorize is a lot faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy\n",
      "28.4 ms ± 11.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "CPU vectorize\n",
      "31.5 ms ± 35.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "CPU vectorize (multicore)\n",
      "3.97 ms ± 63.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "GPU vectorize\n",
      "6.32 ms ± 168 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy\")\n",
    "%timeit np.sin(x) * np.cos(y)\n",
    "\n",
    "print(\"CPU vectorize\")\n",
    "%timeit cpu_sincos(x, y)\n",
    "\n",
    "print(\"CPU vectorize (multicore)\")\n",
    "%timeit cpu_sincos_mc(x, y)\n",
    "\n",
    "print(\"GPU vectorize\")\n",
    "%timeit gpu_sincos(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The data must be transfer from CPU ram to the GPU global memory. The results is written back from the GPU global memory to CPU ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the objects\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behind teh scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic memory transfer\n",
    "- Numpy arrays are automatically transferred\n",
    "    - CPU -> GPU\n",
    "    - GPU -> CPU\n",
    "    \n",
    "### Automatic work scheduling\n",
    "- The work is distributed the across all threads on the GPU\n",
    "- The GPU hardware handles the scheduling\n",
    "\n",
    "### Automatic GPU memory management\n",
    "- GPU memory is tied to object lifetime\n",
    "- freed automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Vectorize Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.vectorize(\n",
    "    ['float32(float32, float32, float32, float32)'],\n",
    "    target = \"cpu\")\n",
    "def cpu_powers(p, q, r, s):\n",
    "    return math.sqrt(p**2 + q**3 + r**4 + s**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.vectorize(\n",
    "    ['float32(float32, float32, float32, float32)'],\n",
    "    target = \"cuda\")\n",
    "def gpu_powers(p, q, r, s):\n",
    "    return math.sqrt(p**2 + q**3 + r**4 + s**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU vectorize correct:  True\n",
      "GPU vectorize correct:  True\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "n = 5000000\n",
    "p = np.random.random(n).astype(np.float32)\n",
    "q = np.random.random(n).astype(np.float32)\n",
    "r = np.random.random(n).astype(np.float32)\n",
    "s = np.random.random(n).astype(np.float32)\n",
    "\n",
    "# Check results\n",
    "np_ans = np.sqrt(p**2 + q**3 + r**4 + s**5)\n",
    "cpu_ans = cpu_powers(p, q, r, s)\n",
    "gpu_ans = gpu_powers(p, q, r, s)\n",
    "\n",
    "print(\"CPU vectorize correct: \", np.allclose(cpu_ans, np_ans))\n",
    "print(\"GPU vectorize correct: \", np.allclose(gpu_ans, np_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy\n",
      "679 ms ± 5.85 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "CPU vectorize\n",
      "9.82 ms ± 16.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "GPU vectorize\n",
      "21.7 ms ± 422 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy\")\n",
    "%timeit np.sqrt(p**2 + q**3 + r**4 + s**5)\n",
    "\n",
    "print(\"CPU vectorize\")\n",
    "%timeit cpu_powers(p, q, r, s)\n",
    "\n",
    "print(\"GPU vectorize\")\n",
    "%timeit gpu_powers(p, q, r, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder why in my examples, the GPU vectorize is slower than the CPU vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del p, q, r, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Universal Function (guvectorize)\n",
    "- vectorize is limited to scalar arguments in the core function\n",
    "- GUVectorize accepts array arguments\n",
    "\n",
    "reference\n",
    "- https://numba.pydata.org/numba-doc/latest/cuda/ufunc.html\n",
    "- https://numba.pydata.org/numba-doc/dev/user/vectorize.html\n",
    "\n",
    "\n",
    "\"While **vectorize()** allows you to write ufuncs that work on one element at a time, the guvectorize() decorator takes the concept one step further and allows you to write ufuncs that will work on an arbitrary number of elements of input arrays, and take and return arrays of differing dimensions. The typical example is a running median or a convolution filter.\n",
    "\n",
    "Contrary to vectorize() functions, guvectorize() functions don’t return their result value: they take it as an array argument, which must be filled in by the function. This is because the array is actually allocated by NumPy’s dispatch mechanism, which calls into the Numba-generated code.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.guvectorize(\n",
    "    ['void(float32[:, :], float32[:, :], float32[:, :])'],\n",
    "    '(m, n), (n, p) -> (n, p)', \n",
    "    target = \"cuda\")\n",
    "def batch_matrix_mult(a, b, c):\n",
    "    for i in range(c.shape[0]):\n",
    "        for j in range(c.shape[1]):\n",
    "            tmp = 0\n",
    "            for n in range(a.shape[1]):\n",
    "                tmp += a[i, n] * b[n, j]\n",
    "            c[i, j] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy result\n",
      "[[ 30.  36.  42.]\n",
      " [ 66.  81.  96.]\n",
      " [102. 126. 150.]]\n",
      "NumPy GPU result\n",
      "[[ 30.  36.  42.]\n",
      " [ 66.  81.  96.]\n",
      " [102. 126. 150.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1.0, 10.0, dtype = np.float32).reshape(3, 3)\n",
    "b = np.arange(1.0, 10.0, dtype = np.float32).reshape(3, 3)\n",
    "\n",
    "# use the builtin matrix multiply in numpy for CPU test\n",
    "import numpy.core.umath_tests as ut\n",
    "\n",
    "# Check result\n",
    "print(\"NumPy result\")\n",
    "np_ans = ut.matrix_multiply(a, b)\n",
    "print(np_ans)\n",
    "\n",
    "print(\"NumPy GPU result\")\n",
    "gpu_ans = batch_matrix_mult(a, b)\n",
    "print(gpu_ans)\n",
    "\n",
    "assert np.allclose(np_ans, gpu_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "Test it out\n",
    "- Batch multiply two 4 million 2x2 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy time\n",
      "77.6 ms ± 171 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Numpy GPU time\n",
      "120 ms ± 885 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "n = 4000000\n",
    "dim = 2\n",
    "a = np.random.random(n * dim * dim). \\\n",
    "    astype(np.float32). \\\n",
    "    reshape(n, dim, dim)\n",
    "\n",
    "b = np.random.random(n * dim * dim). \\\n",
    "    astype(np.float32). \\\n",
    "    reshape(n, dim, dim)\n",
    "\n",
    "print(\"Numpy time\")\n",
    "%timeit ut.matrix_multiply(a, b)\n",
    "\n",
    "print(\"Numpy GPU time\")\n",
    "%timeit batch_matrix_mult(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU time seems to be similar to CPU time (in my example, the GPU time is even higher than CPU time)\n",
    "- It is because the memory transfer\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Transer the data to the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will let us see the actual compute time without the CPU <-> GPU transfer\n",
    "- **numba.cuda.device_array_like** allocate without initialization with the type and shape of another array.\n",
    "    - similar to numpy.empty_like(a)\n",
    "- **numba.cuda.to_device** create a GPU copy of the CPU array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this does not copy a to GPU\n",
    "dc = numba.cuda.device_array_like(a)\n",
    "\n",
    "# copy\n",
    "da = numba.cuda.to_device(a)\n",
    "db = numba.cuda.to_device(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 µs ± 1.28 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def check_pure_compute_time(da, db, dc):\n",
    "    batch_matrix_mult(da, db, out = dc)\n",
    "    numba.cuda.synchronize() # ensure the call has completed\n",
    "    \n",
    "%timeit check_pure_compute_time(da, db, dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- actual compute time is a lot faster\n",
    "- PCI-express transfer overhead\n",
    "\n",
    "**Tips**  \n",
    "If you have a sequence of ufuncs to apply, pin the data on the GPU by manual transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumbaPro CUDA Libraries\n",
    "- Access to CUDA libraries\n",
    "- Work seamless with Numpy\n",
    "    - auto memory transfer\n",
    "    - managed memory\n",
    "- cuBLAS: CUDA version of BLAS\n",
    "- cuSparse: CUDA sparse matrix support\n",
    "- cuFFT: FFT on CUDA\n",
    "- cuRNAD: random number generation on CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example with CUDA lib\n",
    "- Convolutionon GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import cuFFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skcuda import cufft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import fftconvolve \n",
    "from scipy import misc, ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build elementwise complex array multiplication CUDA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.vectorize(['complex64(complex64, complex64)'], target = \"cuda\")\n",
    "def vmult(a, b):\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare image and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = misc.face(gray = True).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4., -1.,  0., -1., -4.],\n",
       "       [-1.,  2.,  3.,  2., -1.],\n",
       "       [ 0.,  3.,  4.,  3.,  0.],\n",
       "       [-1.,  2.,  3.,  2., -1.],\n",
       "       [-4., -1.,  0., -1., -4.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplacian_pts = '''\n",
    "-4 -1 0 -1 -4\n",
    "-1 2 3 2 -1\n",
    "0 3 4 3 0\n",
    "-1 2 3 2 -1\n",
    "-4 -1 0 -1 -4\n",
    "'''.split()\n",
    "laplacian = np.array(laplacian_pts, dtype = np.float32).reshape(5, 5)\n",
    "laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = np.zeros_like(image)\n",
    "response[:5, :5] = laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (768, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"Image size: %s\" % (image.shape, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 0.06s\n"
     ]
    }
   ],
   "source": [
    "ts = timer() # start Timer\n",
    "cvimage_cpu = fftconvolve(image, laplacian, mode = \"same\")\n",
    "te = timer() # stop Timer\n",
    "print('CPU: %.2fs' % (te - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_complex = image.astype(np.complex64)\n",
    "response_complex = response.astype(np.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'skcuda.cufft' has no attribute 'fft_inplace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-64073b03b331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Forward FFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcufft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_image_complex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcufft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_response_complex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'skcuda.cufft' has no attribute 'fft_inplace'"
     ]
    }
   ],
   "source": [
    "ts = timer() # start Timer\n",
    "\n",
    "d_image_complex = numba.cuda.to_device(image_complex)\n",
    "d_response_complex = numba.cuda.to_device(response_complex)\n",
    "\n",
    "# Forward FFT\n",
    "cufft.fft_inplace(d_image_complex)\n",
    "cufft.fft_inplace(d_response_complex)\n",
    "\n",
    "# Multiply the image with the filter\n",
    "vmult(d_image_complex, d_response_complex, out = d_image_complex)\n",
    "\n",
    "# Inverse FFT\n",
    "cufft.ifft_inplace(d_image_complex)\n",
    "cvimage_gpu = d_image_complex.copy_to_host().real / np.prod(image.shape)\n",
    "\n",
    "te = timer() # stop Timer\n",
    "print('GPU: %.2fs' % (te - ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-level Approach: @numba.cuda.jit\n",
    "- Numba can generate CUDA functions with the @jit decorator\n",
    "- decorated function follows CUDA execution model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Execution Model\n",
    "- Kernel functions (created when you decorate a function)\n",
    "    - visible to the host CPU\n",
    "    - connot return any value\n",
    "        - use output argument\n",
    "    - associates to a grid\n",
    "- Grid\n",
    "    - a group of blocks\n",
    "    - 1D, 2D, 3D\n",
    "- Blocks\n",
    "    - a group of threads\n",
    "    - 1D, 2D, 3D\n",
    "- Every thread executes the same kernel\n",
    "    - thread can use the grid, block, thread coordinate system to determines its ID    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://cuda.ce.rit.edu/cuda_overview/clip_image004.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling a CUDA Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import numba\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<macro tid.x -> () -> int32>\n",
      "<macro ctaid.x -> () -> int32>\n",
      "<macro ntid.x -> () -> int32>\n"
     ]
    }
   ],
   "source": [
    "print(cuda.threadIdx.x)\n",
    "print(cuda.blockIdx.x)\n",
    "print(cuda.blockDim.x)   # number of threads per block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(\"void(float32[:], float32[:], float32[:])\")\n",
    "def vadd(arr_a, arr_b, arr_out):\n",
    "    # thread coordinate system\n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x\n",
    "    \n",
    "    # flatten the coordinate system into indices\n",
    "    i = tx + bx + bw\n",
    "    \n",
    "    # why are we checking (see later in notebooks)\n",
    "    if i >= arr_out.size:\n",
    "        return\n",
    "    arr_out[i] = arr_a[i] + arr_b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: why does the kernel function cannot return any value?**\n",
    "execution -> map to the cuda execution model    \n",
    "the hardware of CUDA pose an restrictions  \n",
    "one restriction is the output must be passed as an argument  \n",
    "\n",
    "when you call up kernel, after the launch, the kernel is actually not finished and therefore you will never get the return value\n",
    "\n",
    "<s>synchronizationator</s> read the output from the global memory of GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a CUDA kernel with three 1D float32 arrays as args**\n",
    "```\n",
    "@cuda.jit(\"void(float32[:], float32[:], float32[:])\")\n",
    "def vadd(arr_a, arr_b, arr_out):\n",
    "```\n",
    "\n",
    "**Map thread, block coordinate to global position**\n",
    "```\n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x\n",
    "    i = tx + bx + bw\n",
    "```\n",
    "or simplified to:\n",
    "```\n",
    "    i = cuda.grid(1)\n",
    "```\n",
    "\n",
    "**Ensure global position is within array size**\n",
    "```\n",
    "    if i >= arr_out.size:\n",
    "        return\n",
    "```\n",
    "\n",
    "**The actual work**\n",
    "```\n",
    "    arr_out[i] = arr_a[i] + arr_b[i]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "a = np.arange(n, dtype = np.float32)\n",
    "b = np.arange(n, dtype = np.float32)\n",
    "c = np.empty_like(a) # Must prepare the output array to hold the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate thread, block count**\n",
    "- thread count is set to warp size of teh GPU\n",
    "    - Warp size is similar to SIMD vector width on the CPU\n",
    "    - performance tips: set thread count to multiple of warp size\n",
    "- block count is ceil(n / thread_ct)\n",
    "\n",
    "**Note:** This will lauch more threads than there are elements in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads per block: 32\n",
      "Block per grid: 4\n"
     ]
    }
   ],
   "source": [
    "my_gpu = numba.cuda.get_current_device()\n",
    "thread_ct = my_gpu.WARP_SIZE\n",
    "block_ct = int(math.ceil(float(n) / thread_ct))\n",
    "\n",
    "print(\"Threads per block:\", thread_ct)\n",
    "print(\"Block per grid:\", block_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.,\n",
       "        22.,  24.,  26.,  28.,  30.,  32.,  34.,  36.,  38.,  40.,  42.,\n",
       "        44.,  46.,  48.,  50.,  52.,  54.,  56.,  58.,  60.,  62.,  64.,\n",
       "        66.,  68.,  70.,  72.,  74.,  76.,  78.,  80.,  82.,  84.,  86.,\n",
       "        88.,  90.,  92.,  94.,  96.,  98., 100., 102., 104., 106., 108.,\n",
       "       110., 112., 114., 116., 118., 120., 122., 124., 126., 128., 130.,\n",
       "       132., 134., 136., 138., 140., 142., 144., 146., 148., 150., 152.,\n",
       "       154., 156., 158., 160., 162., 164., 166., 168., 170., 172., 174.,\n",
       "       176., 178., 180., 182., 184., 186., 188., 190., 192., 194., 196.,\n",
       "       198.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  64.  66.  68.  70.  72.  74.  76.  78.  80.  82.\n",
      "  84.  86.  88.  90.  92.  94.  96.  98. 100. 102. 104. 106. 108. 110.\n",
      " 112. 114. 116. 118. 120. 122. 124. 126. 128. 130. 132.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "  98.  99.]\n"
     ]
    }
   ],
   "source": [
    "# Last argument is the output array in this case\n",
    "vadd[block_ct, thread_ct](a, b, c)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are not correct, however, it works properly if I change the code into the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(\"void(float32[:], float32[:], float32[:])\")\n",
    "def vadd(arr_a, arr_b, arr_out):\n",
    "    # thread coordinate system\n",
    "    #tx = cuda.threadIdx.x\n",
    "    #bx = cuda.blockIdx.x\n",
    "    #bw = cuda.blockDim.x\n",
    "    \n",
    "    # flatten the coordinate system into indices\n",
    "    #i = tx + bx + bw\n",
    "    i = cuda.grid(1)\n",
    "    \n",
    "    # why are we checking (see later in notebooks)\n",
    "    #if i >= arr_out.size:\n",
    "    #    return\n",
    "    arr_out[i] = arr_a[i] + arr_b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   2.   4.   6.   8.  10.  12.  14.  16.  18.  20.  22.  24.  26.\n",
      "  28.  30.  32.  34.  36.  38.  40.  42.  44.  46.  48.  50.  52.  54.\n",
      "  56.  58.  60.  62.  64.  66.  68.  70.  72.  74.  76.  78.  80.  82.\n",
      "  84.  86.  88.  90.  92.  94.  96.  98. 100. 102. 104. 106. 108. 110.\n",
      " 112. 114. 116. 118. 120. 122. 124. 126. 128. 130. 132. 134. 136. 138.\n",
      " 140. 142. 144. 146. 148. 150. 152. 154. 156. 158. 160. 162. 164. 166.\n",
      " 168. 170. 172. 174. 176. 178. 180. 182. 184. 186. 188. 190. 192. 194.\n",
      " 196. 198.]\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "a = np.arange(n, dtype = np.float32)\n",
    "b = np.arange(n, dtype = np.float32)\n",
    "c = np.empty_like(a) # Must prepare the output array to hold the result\n",
    "\n",
    "# Last argument is the output array in this case\n",
    "vadd[block_ct, thread_ct](a, b, c)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Matrix-matrix multiplication\n",
    "- Show manual caching with shared memory\n",
    "- Not the best matrix matrix multiplication implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import float32\n",
    "\n",
    "bpg = 150\n",
    "tpb = 32\n",
    "n = bpg + tpb\n",
    "shared_mem_size = (tpb, tpb)\n",
    "griddim = bpg, bpg\n",
    "blackdim = tpb, tpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def increment_a_2D_array(an_array):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x < an_array.shape[0] and y < an_array.shape[1]:\n",
    "        an_array[x, y] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed at nopython (nopython frontend)\nInternal error at <numba.typeinfer.StaticGetItemConstraint object at 0x7f98341d03c8>:\n--%<-----------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/numba/errors.py\", line 243, in new_error_context\n    yield\n  File \"/usr/lib/python3/dist-packages/numba/typeinfer.py\", line 321, in __call__\n    index=self.index)\n  File \"/usr/lib/python3/dist-packages/numba/typing/context.py\", line 249, in resolve_static_getitem\n    return self.resolve_function_type(\"static_getitem\", args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/context.py\", line 189, in resolve_function_type\n    res = defn.apply(args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/templates.py\", line 193, in apply\n    sig = generic(args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/builtins.py\", line 526, in generic\n    return tup.types[idx]\nIndexError: tuple index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/numba/typeinfer.py\", line 129, in propagate\n    constraint(typeinfer)\n  File \"/usr/lib/python3/dist-packages/numba/typeinfer.py\", line 325, in __call__\n    self.fallback(typeinfer)\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/usr/lib/python3/dist-packages/numba/errors.py\", line 249, in new_error_context\n    six.reraise(type(newerr), newerr, sys.exc_info()[2])\n  File \"/usr/lib/python3/dist-packages/numba/six.py\", line 658, in reraise\n    raise value.with_traceback(tb)\n  File \"/usr/lib/python3/dist-packages/numba/errors.py\", line 243, in new_error_context\n    yield\n  File \"/usr/lib/python3/dist-packages/numba/typeinfer.py\", line 321, in __call__\n    index=self.index)\n  File \"/usr/lib/python3/dist-packages/numba/typing/context.py\", line 249, in resolve_static_getitem\n    return self.resolve_function_type(\"static_getitem\", args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/context.py\", line 189, in resolve_function_type\n    res = defn.apply(args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/templates.py\", line 193, in apply\n    sig = generic(args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/builtins.py\", line 526, in generic\n    return tup.types[idx]\nnumba.errors.InternalError: tuple index out of range\n[1] During: typing of static-get-item at <ipython-input-21-6be631d667d1> (6)\n--%<-----------------------------------------------------------------\n\nFile \"<ipython-input-21-6be631d667d1>\", line 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6be631d667d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"void(float32[:], float32[:], float32[:])\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnaive_matrix_mult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#if (x >= n) or (y >= n):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#    return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/cuda/decorators.py\u001b[0m in \u001b[0;36mkernel_jit\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mkernel_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             kernel = compile_kernel(func, argtypes, link=link, debug=debug,\n\u001b[0;32m---> 92\u001b[0;31m                                     inline=inline, fastmath=fastmath)\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# Force compilation for the current context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcore\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_cuda_compiler_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_kernel\u001b[0;34m(pyfunc, args, link, debug, inline, fastmath)\u001b[0m\n\u001b[1;32m     73\u001b[0m def compile_kernel(pyfunc, args, link, debug=False, inline=False,\n\u001b[1;32m     74\u001b[0m                    fastmath=False):\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mcres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfndesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllvm_func_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     lib, kernel = cres.target_context.prepare_cuda_kernel(cres.library, fname,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcore\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_cuda_compiler_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_cuda\u001b[0;34m(pyfunc, return_type, args, debug, inline)\u001b[0m\n\u001b[1;32m     62\u001b[0m                                   \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                   \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                   locals={})\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mlibrary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/compiler.py\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library)\u001b[0m\n\u001b[1;32m    738\u001b[0m     pipeline = Pipeline(typingctx, targetctx, library,\n\u001b[1;32m    739\u001b[0m                         args, return_type, flags, locals)\n\u001b[0;32m--> 740\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_extra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/compiler.py\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifted_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_bytecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompile_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_ir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifted_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/compiler.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \"\"\"\n\u001b[1;32m    698\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_ir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compile_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/compiler.py\u001b[0m in \u001b[0;36m_compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0;31m# Early pipeline completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/compiler.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, status)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;31m# No more fallback pipelines?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_final_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpatched_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                     \u001b[0;31m# Go to next fallback pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/compiler.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, status)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                     \u001b[0mstage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0m_EarlyPipelineCompletion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/compiler.py\u001b[0m in \u001b[0;36mstage_nopython_frontend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 self.locals)\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         with self.fallback_context('Function \"%s\" has invalid return type'\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/compiler.py\u001b[0m in \u001b[0;36mtype_inference_stage\u001b[0;34m(typingctx, interp, args, return_type, locals)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_constraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         \u001b[0mtypemap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalltypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numba/typeinfer.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, raise_errors)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed at nopython (nopython frontend)\nInternal error at <numba.typeinfer.StaticGetItemConstraint object at 0x7f98341d03c8>:\n--%<-----------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/numba/errors.py\", line 243, in new_error_context\n    yield\n  File \"/usr/lib/python3/dist-packages/numba/typeinfer.py\", line 321, in __call__\n    index=self.index)\n  File \"/usr/lib/python3/dist-packages/numba/typing/context.py\", line 249, in resolve_static_getitem\n    return self.resolve_function_type(\"static_getitem\", args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/context.py\", line 189, in resolve_function_type\n    res = defn.apply(args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/templates.py\", line 193, in apply\n    sig = generic(args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/builtins.py\", line 526, in generic\n    return tup.types[idx]\nIndexError: tuple index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/numba/typeinfer.py\", line 129, in propagate\n    constraint(typeinfer)\n  File \"/usr/lib/python3/dist-packages/numba/typeinfer.py\", line 325, in __call__\n    self.fallback(typeinfer)\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/usr/lib/python3/dist-packages/numba/errors.py\", line 249, in new_error_context\n    six.reraise(type(newerr), newerr, sys.exc_info()[2])\n  File \"/usr/lib/python3/dist-packages/numba/six.py\", line 658, in reraise\n    raise value.with_traceback(tb)\n  File \"/usr/lib/python3/dist-packages/numba/errors.py\", line 243, in new_error_context\n    yield\n  File \"/usr/lib/python3/dist-packages/numba/typeinfer.py\", line 321, in __call__\n    index=self.index)\n  File \"/usr/lib/python3/dist-packages/numba/typing/context.py\", line 249, in resolve_static_getitem\n    return self.resolve_function_type(\"static_getitem\", args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/context.py\", line 189, in resolve_function_type\n    res = defn.apply(args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/templates.py\", line 193, in apply\n    sig = generic(args, kws)\n  File \"/usr/lib/python3/dist-packages/numba/typing/builtins.py\", line 526, in generic\n    return tup.types[idx]\nnumba.errors.InternalError: tuple index out of range\n[1] During: typing of static-get-item at <ipython-input-21-6be631d667d1> (6)\n--%<-----------------------------------------------------------------\n\nFile \"<ipython-input-21-6be631d667d1>\", line 6"
     ]
    }
   ],
   "source": [
    "@cuda.jit(\"void(float32[:], float32[:], float32[:])\")\n",
    "def naive_matrix_mult(A, B, C):\n",
    "    x, y = cuda.grid(2)\n",
    "    #if (x >= n) or (y >= n):\n",
    "    #    return\n",
    "    if x < C.shape[0] and y < C.shape[1]:\n",
    "        C[x, y] += 1\n",
    "    #C[y, x] = 0.0\n",
    "    #for i in range(n):\n",
    "    #    C[y, x] += A[y, i] * B[i, x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
